{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1444189,
     "status": "ok",
     "timestamp": 1762163011484,
     "user": {
      "displayName": "Chandan B L",
      "userId": "07233415136554425709"
     },
     "user_tz": -330
    },
    "id": "Ayvbw7Nto-uU",
    "outputId": "430e6d1a-dbed-4113-9718-16fea57a14ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYBRID HMM-RL SOLUTION\n",
      "============================================================\n",
      "Loading corpus...\n",
      "Loaded 49979 valid words\n",
      "Calculating bigram probabilities...\n",
      "Bigram probabilities calculated\n",
      "Calculating trigram probabilities...\n",
      "Trigram probabilities calculated\n",
      "\n",
      "Training Enhanced HMM models with trigrams...\n",
      "Training HMM for word length 11 with 5452 words\n",
      "Training HMM for word length 6 with 3755 words\n",
      "Training HMM for word length 9 with 6787 words\n",
      "Training HMM for word length 16 with 698 words\n",
      "Training HMM for word length 14 with 2019 words\n",
      "Training HMM for word length 10 with 6465 words\n",
      "Training HMM for word length 8 with 6348 words\n",
      "Training HMM for word length 12 with 4292 words\n",
      "Training HMM for word length 13 with 3094 words\n",
      "Training HMM for word length 5 with 2340 words\n",
      "Training HMM for word length 18 with 174 words\n",
      "Training HMM for word length 4 with 1169 words\n",
      "Training HMM for word length 3 with 388 words\n",
      "Training HMM for word length 7 with 5111 words\n",
      "Training HMM for word length 15 with 1226 words\n",
      "Training HMM for word length 17 with 375 words\n",
      "Training HMM for word length 22 with 8 words\n",
      "Training HMM for word length 19 with 88 words\n",
      "Training HMM for word length 2 with 84 words\n",
      "Training HMM for word length 1 with 46 words\n",
      "Training HMM for word length 20 with 40 words\n",
      "Training HMM for word length 21 with 16 words\n",
      "Trained HMMs for 22 different word lengths\n",
      "\n",
      "==================================================\n",
      "CORPUS ANALYSIS\n",
      "==================================================\n",
      "\n",
      "Top 15 most common letters:\n",
      "e: 49203 (10.37%)\n",
      "a: 42089 (8.87%)\n",
      "i: 42047 (8.86%)\n",
      "o: 35808 (7.54%)\n",
      "r: 33577 (7.07%)\n",
      "n: 33314 (7.02%)\n",
      "t: 32191 (6.78%)\n",
      "s: 29044 (6.12%)\n",
      "l: 27406 (5.77%)\n",
      "c: 21718 (4.58%)\n",
      "u: 18376 (3.87%)\n",
      "p: 16426 (3.46%)\n",
      "m: 14670 (3.09%)\n",
      "d: 14324 (3.02%)\n",
      "h: 13643 (2.87%)\n",
      "\n",
      "Word length statistics:\n",
      "Min: 1, Max: 24, Avg: 9.50\n",
      "\n",
      "HMM coverage: 22 different word lengths\n",
      "Length 1: 46 words\n",
      "Length 2: 84 words\n",
      "Length 3: 388 words\n",
      "Length 4: 1169 words\n",
      "Length 5: 2340 words\n",
      "Length 6: 3755 words\n",
      "Length 7: 5111 words\n",
      "Length 8: 6348 words\n",
      "Length 9: 6787 words\n",
      "Length 10: 6465 words\n",
      "Length 11: 5452 words\n",
      "Length 12: 4292 words\n",
      "Length 13: 3094 words\n",
      "Length 14: 2019 words\n",
      "Length 15: 1226 words\n",
      "Length 16: 698 words\n",
      "Length 17: 375 words\n",
      "Length 18: 174 words\n",
      "Length 19: 88 words\n",
      "Length 20: 40 words\n",
      "Length 21: 16 words\n",
      "Length 22: 8 words\n",
      "RL Agent using device: cuda\n",
      "\n",
      "============================================================\n",
      "TRAINING RL AGENT\n",
      "============================================================\n",
      "Training RL agent for 10000 episodes...\n",
      "Episode 100, Average Reward: -2.53, Epsilon: 0.943\n",
      "Episode 200, Average Reward: -3.78, Epsilon: 0.888\n",
      "Episode 300, Average Reward: -3.80, Epsilon: 0.837\n",
      "Episode 400, Average Reward: -1.53, Epsilon: 0.788\n",
      "Episode 500, Average Reward: -3.06, Epsilon: 0.742\n",
      "Episode 600, Average Reward: -2.42, Epsilon: 0.699\n",
      "Episode 700, Average Reward: -1.91, Epsilon: 0.658\n",
      "Episode 800, Average Reward: -0.34, Epsilon: 0.620\n",
      "Episode 900, Average Reward: -1.54, Epsilon: 0.584\n",
      "Episode 1000, Average Reward: -2.05, Epsilon: 0.550\n",
      "Episode 1100, Average Reward: -2.16, Epsilon: 0.518\n",
      "Episode 1200, Average Reward: -2.83, Epsilon: 0.488\n",
      "Episode 1300, Average Reward: -1.58, Epsilon: 0.459\n",
      "Episode 1400, Average Reward: -0.43, Epsilon: 0.432\n",
      "Episode 1500, Average Reward: -0.31, Epsilon: 0.407\n",
      "Episode 1600, Average Reward: 0.76, Epsilon: 0.383\n",
      "Episode 1700, Average Reward: -0.85, Epsilon: 0.361\n",
      "Episode 1800, Average Reward: -0.86, Epsilon: 0.340\n",
      "Episode 1900, Average Reward: -0.76, Epsilon: 0.320\n",
      "Episode 2000, Average Reward: -2.06, Epsilon: 0.302\n",
      "Episode 2100, Average Reward: -0.49, Epsilon: 0.284\n",
      "Episode 2200, Average Reward: -0.72, Epsilon: 0.268\n",
      "Episode 2300, Average Reward: 2.11, Epsilon: 0.252\n",
      "Episode 2400, Average Reward: 2.14, Epsilon: 0.237\n",
      "Episode 2500, Average Reward: -0.25, Epsilon: 0.223\n",
      "Episode 2600, Average Reward: -0.26, Epsilon: 0.210\n",
      "Episode 2700, Average Reward: 0.85, Epsilon: 0.198\n",
      "Episode 2800, Average Reward: 3.04, Epsilon: 0.187\n",
      "Episode 2900, Average Reward: -1.04, Epsilon: 0.176\n",
      "Episode 3000, Average Reward: -0.88, Epsilon: 0.166\n",
      "Episode 3100, Average Reward: -0.89, Epsilon: 0.156\n",
      "Episode 3200, Average Reward: 1.00, Epsilon: 0.147\n",
      "Episode 3300, Average Reward: 1.17, Epsilon: 0.138\n",
      "Episode 3400, Average Reward: -0.95, Epsilon: 0.130\n",
      "Episode 3500, Average Reward: -3.19, Epsilon: 0.123\n",
      "Episode 3600, Average Reward: -3.08, Epsilon: 0.115\n",
      "Episode 3700, Average Reward: -2.27, Epsilon: 0.109\n",
      "Episode 3800, Average Reward: -1.74, Epsilon: 0.102\n",
      "Episode 3900, Average Reward: -0.17, Epsilon: 0.096\n",
      "Episode 4000, Average Reward: -0.85, Epsilon: 0.091\n",
      "Episode 4100, Average Reward: -4.10, Epsilon: 0.086\n",
      "Episode 4200, Average Reward: -0.23, Epsilon: 0.081\n",
      "Episode 4300, Average Reward: -2.02, Epsilon: 0.076\n",
      "Episode 4400, Average Reward: 0.60, Epsilon: 0.071\n",
      "Episode 4500, Average Reward: 1.68, Epsilon: 0.067\n",
      "Episode 4600, Average Reward: 2.23, Epsilon: 0.063\n",
      "Episode 4700, Average Reward: 2.97, Epsilon: 0.060\n",
      "Episode 4800, Average Reward: -1.87, Epsilon: 0.056\n",
      "Episode 4900, Average Reward: 0.66, Epsilon: 0.053\n",
      "Episode 5000, Average Reward: 0.67, Epsilon: 0.050\n",
      "Episode 5100, Average Reward: -0.30, Epsilon: 0.047\n",
      "Episode 5200, Average Reward: 1.25, Epsilon: 0.044\n",
      "Episode 5300, Average Reward: 0.67, Epsilon: 0.042\n",
      "Episode 5400, Average Reward: -0.32, Epsilon: 0.039\n",
      "Episode 5500, Average Reward: 0.43, Epsilon: 0.037\n",
      "Episode 5600, Average Reward: -0.30, Epsilon: 0.035\n",
      "Episode 5700, Average Reward: -1.49, Epsilon: 0.033\n",
      "Episode 5800, Average Reward: -0.55, Epsilon: 0.031\n",
      "Episode 5900, Average Reward: -1.91, Epsilon: 0.029\n",
      "Episode 6000, Average Reward: 1.74, Epsilon: 0.027\n",
      "Episode 6100, Average Reward: 3.12, Epsilon: 0.026\n",
      "Episode 6200, Average Reward: 2.24, Epsilon: 0.024\n",
      "Episode 6300, Average Reward: 4.91, Epsilon: 0.023\n",
      "Episode 6400, Average Reward: 2.75, Epsilon: 0.022\n",
      "Episode 6500, Average Reward: 3.65, Epsilon: 0.020\n",
      "Episode 6600, Average Reward: 3.27, Epsilon: 0.019\n",
      "Episode 6700, Average Reward: 4.68, Epsilon: 0.018\n",
      "Episode 6800, Average Reward: 5.88, Epsilon: 0.017\n",
      "Episode 6900, Average Reward: 6.65, Epsilon: 0.016\n",
      "Episode 7000, Average Reward: 6.55, Epsilon: 0.015\n",
      "Episode 7100, Average Reward: 5.69, Epsilon: 0.014\n",
      "Episode 7200, Average Reward: 8.16, Epsilon: 0.013\n",
      "Episode 7300, Average Reward: 8.12, Epsilon: 0.013\n",
      "Episode 7400, Average Reward: 7.96, Epsilon: 0.012\n",
      "Episode 7500, Average Reward: 8.98, Epsilon: 0.011\n",
      "Episode 7600, Average Reward: 8.88, Epsilon: 0.010\n",
      "Episode 7700, Average Reward: 7.97, Epsilon: 0.010\n",
      "Episode 7800, Average Reward: 8.96, Epsilon: 0.010\n",
      "Episode 7900, Average Reward: 7.11, Epsilon: 0.010\n",
      "Episode 8000, Average Reward: 8.39, Epsilon: 0.010\n",
      "Episode 8100, Average Reward: 9.93, Epsilon: 0.010\n",
      "Episode 8200, Average Reward: 8.91, Epsilon: 0.010\n",
      "Episode 8300, Average Reward: 9.22, Epsilon: 0.010\n",
      "Episode 8400, Average Reward: 6.91, Epsilon: 0.010\n",
      "Episode 8500, Average Reward: 9.84, Epsilon: 0.010\n",
      "Episode 8600, Average Reward: 10.09, Epsilon: 0.010\n",
      "Episode 8700, Average Reward: 10.37, Epsilon: 0.010\n",
      "Episode 8800, Average Reward: 7.92, Epsilon: 0.010\n",
      "Episode 8900, Average Reward: 10.37, Epsilon: 0.010\n",
      "Episode 9000, Average Reward: 8.80, Epsilon: 0.010\n",
      "Episode 9100, Average Reward: 4.68, Epsilon: 0.010\n",
      "Episode 9200, Average Reward: 6.22, Epsilon: 0.010\n",
      "Episode 9300, Average Reward: 3.16, Epsilon: 0.010\n",
      "Episode 9400, Average Reward: 2.69, Epsilon: 0.010\n",
      "Episode 9500, Average Reward: 3.67, Epsilon: 0.010\n",
      "Episode 9600, Average Reward: 5.08, Epsilon: 0.010\n",
      "Episode 9700, Average Reward: 4.12, Epsilon: 0.010\n",
      "Episode 9800, Average Reward: 5.44, Epsilon: 0.010\n",
      "Episode 9900, Average Reward: 6.68, Epsilon: 0.010\n",
      "Episode 10000, Average Reward: 6.63, Epsilon: 0.010\n",
      "\n",
      "============================================================\n",
      "DETAILED RL-HYBRID DEMO\n",
      "============================================================\n",
      "\n",
      "Playing with word: python (RL mode - Epsilon=0 for Demo)\n",
      "\n",
      "Target word: python\n",
      "Starting game: _ _ _ _ _ _\n",
      "--- Step 1 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      e: 0.1105\n",
      "      a: 0.1005\n",
      "      i: 0.0724\n",
      "      r: 0.0711\n",
      "      n: 0.0630\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.5907\n",
      "      e: 0.5545\n",
      "      i: 0.5356\n",
      "      l: 0.2990\n",
      "      v: 0.2774\n",
      "  -> Agent's Final Choice: 'a'\n",
      "  Wrong! Mistakes: 1/6\n",
      "--- Step 2 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      e: 0.1228\n",
      "      i: 0.0805\n",
      "      r: 0.0791\n",
      "      n: 0.0700\n",
      "      o: 0.0690\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      e: 0.5669\n",
      "      i: 0.5437\n",
      "      a: 0.4902\n",
      "      l: 0.3057\n",
      "      v: 0.2785\n",
      "  -> Agent's Final Choice: 'e'\n",
      "  Wrong! Mistakes: 2/6\n",
      "--- Step 3 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      i: 0.0918\n",
      "      r: 0.0902\n",
      "      n: 0.0798\n",
      "      o: 0.0787\n",
      "      l: 0.0761\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      i: 0.5549\n",
      "      a: 0.4902\n",
      "      e: 0.4441\n",
      "      l: 0.3151\n",
      "      v: 0.2801\n",
      "  -> Agent's Final Choice: 'i'\n",
      "  Wrong! Mistakes: 3/6\n",
      "--- Step 4 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      r: 0.0993\n",
      "      n: 0.0879\n",
      "      o: 0.0866\n",
      "      l: 0.0838\n",
      "      t: 0.0804\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      l: 0.3228\n",
      "      n: 0.2857\n",
      "  -> Agent's Final Choice: 'l'\n",
      "  Wrong! Mistakes: 4/6\n",
      "--- Step 5 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      r: 0.1084\n",
      "      n: 0.0960\n",
      "      o: 0.0945\n",
      "      t: 0.0877\n",
      "      s: 0.0791\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      n: 0.2938\n",
      "      o: 0.2868\n",
      "  -> Agent's Final Choice: 'n'\n",
      "  Correct! Word: _ _ _ _ _ n\n",
      "--- Step 6 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      o: 0.1234\n",
      "      r: 0.1070\n",
      "      t: 0.0932\n",
      "      s: 0.0838\n",
      "      u: 0.0826\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      o: 0.3157\n",
      "      t: 0.2913\n",
      "  -> Agent's Final Choice: 'o'\n",
      "  Correct! Word: _ _ _ _ o n\n",
      "--- Step 7 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      r: 0.1094\n",
      "      t: 0.1009\n",
      "      s: 0.0858\n",
      "      c: 0.0794\n",
      "      u: 0.0768\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      t: 0.2990\n",
      "      h: 0.2919\n",
      "  -> Agent's Final Choice: 't'\n",
      "  Correct! Word: _ _ t _ o n\n",
      "--- Step 8 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      r: 0.1356\n",
      "      h: 0.1066\n",
      "      u: 0.0844\n",
      "      c: 0.0838\n",
      "      s: 0.0808\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      h: 0.3434\n",
      "      r: 0.2970\n",
      "  -> Agent's Final Choice: 'h'\n",
      "  Correct! Word: _ _ t h o n\n",
      "--- Step 9 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      p: 0.1484\n",
      "      r: 0.1246\n",
      "      d: 0.0771\n",
      "      w: 0.0758\n",
      "      u: 0.0755\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      p: 0.3533\n",
      "      w: 0.3384\n",
      "  -> Agent's Final Choice: 'p'\n",
      "  Correct! Word: p _ t h o n\n",
      "--- Step 10 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      r: 0.2228\n",
      "      u: 0.1277\n",
      "      w: 0.1214\n",
      "      y: 0.0882\n",
      "      d: 0.0864\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      r: 0.3841\n",
      "      w: 0.3840\n",
      "  -> Agent's Final Choice: 'r'\n",
      "  Wrong! Mistakes: 5/6\n",
      "--- Step 11 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      u: 0.1643\n",
      "      w: 0.1561\n",
      "      y: 0.1135\n",
      "      d: 0.1112\n",
      "      v: 0.0995\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      w: 0.4188\n",
      "      u: 0.3674\n",
      "  -> Agent's Final Choice: 'w'\n",
      "  Wrong! Mistakes: 6/6\n",
      " Lost! Word was: python\n",
      "\n",
      "Playing with word: hangman (RL mode - Epsilon=0 for Demo)\n",
      "\n",
      "Target word: hangman\n",
      "Starting game: _ _ _ _ _ _ _\n",
      "--- Step 1 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      e: 0.1075\n",
      "      a: 0.0956\n",
      "      i: 0.0806\n",
      "      r: 0.0714\n",
      "      o: 0.0657\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.5858\n",
      "      e: 0.5516\n",
      "      i: 0.5437\n",
      "      l: 0.2980\n",
      "      v: 0.2759\n",
      "  -> Agent's Final Choice: 'a'\n",
      "  Correct! Word: _ a _ _ _ a _\n",
      "--- Step 2 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      n: 0.1139\n",
      "      l: 0.0871\n",
      "      r: 0.0865\n",
      "      e: 0.0725\n",
      "      t: 0.0722\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      i: 0.5273\n",
      "      e: 0.5166\n",
      "      a: 0.4902\n",
      "      l: 0.3261\n",
      "      n: 0.3117\n",
      "  -> Agent's Final Choice: 'i'\n",
      "  Wrong! Mistakes: 1/6\n",
      "--- Step 3 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      n: 0.1217\n",
      "      l: 0.0931\n",
      "      r: 0.0924\n",
      "      e: 0.0775\n",
      "      t: 0.0771\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      e: 0.5215\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      l: 0.3321\n",
      "      n: 0.3195\n",
      "  -> Agent's Final Choice: 'e'\n",
      "  Wrong! Mistakes: 2/6\n",
      "--- Step 4 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      n: 0.1320\n",
      "      l: 0.1009\n",
      "      r: 0.1002\n",
      "      t: 0.0836\n",
      "      s: 0.0732\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      l: 0.3399\n",
      "      n: 0.3298\n",
      "  -> Agent's Final Choice: 'l'\n",
      "  Wrong! Mistakes: 3/6\n",
      "--- Step 5 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      n: 0.1468\n",
      "      r: 0.1114\n",
      "      t: 0.0930\n",
      "      s: 0.0814\n",
      "      c: 0.0624\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      n: 0.3446\n",
      "      t: 0.2911\n",
      "  -> Agent's Final Choice: 'n'\n",
      "  Correct! Word: _ a n _ _ a n\n",
      "--- Step 6 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      g: 0.1179\n",
      "      t: 0.1150\n",
      "      d: 0.0876\n",
      "      m: 0.0724\n",
      "      s: 0.0714\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      g: 0.3400\n",
      "      d: 0.3172\n",
      "  -> Agent's Final Choice: 'g'\n",
      "  Correct! Word: _ a n g _ a n\n",
      "--- Step 7 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      m: 0.1257\n",
      "      u: 0.0997\n",
      "      o: 0.0908\n",
      "      h: 0.0884\n",
      "      s: 0.0716\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      m: 0.3384\n",
      "      h: 0.3252\n",
      "  -> Agent's Final Choice: 'm'\n",
      "  Correct! Word: _ a n g m a n\n",
      "--- Step 8 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      h: 0.1493\n",
      "      s: 0.1004\n",
      "      t: 0.0757\n",
      "      c: 0.0746\n",
      "      p: 0.0732\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      h: 0.3862\n",
      "      v: 0.2970\n",
      "  -> Agent's Final Choice: 'h'\n",
      "  Correct! Word: h a n g m a n\n",
      "Won in 8 steps!\n",
      "\n",
      "Playing with word: computer (RL mode - Epsilon=0 for Demo)\n",
      "\n",
      "Target word: computer\n",
      "Starting game: _ _ _ _ _ _ _ _\n",
      "--- Step 1 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      e: 0.1086\n",
      "      a: 0.0922\n",
      "      i: 0.0847\n",
      "      r: 0.0716\n",
      "      o: 0.0696\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.5824\n",
      "      e: 0.5526\n",
      "      i: 0.5479\n",
      "      l: 0.2993\n",
      "      v: 0.2768\n",
      "  -> Agent's Final Choice: 'a'\n",
      "  Wrong! Mistakes: 1/6\n",
      "--- Step 2 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      e: 0.1196\n",
      "      i: 0.0934\n",
      "      r: 0.0789\n",
      "      o: 0.0767\n",
      "      n: 0.0728\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      e: 0.5637\n",
      "      i: 0.5565\n",
      "      a: 0.4902\n",
      "      l: 0.3054\n",
      "      v: 0.2778\n",
      "  -> Agent's Final Choice: 'e'\n",
      "  Correct! Word: _ _ _ _ _ _ e _\n",
      "--- Step 3 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      r: 0.1345\n",
      "      d: 0.0867\n",
      "      i: 0.0846\n",
      "      t: 0.0787\n",
      "      n: 0.0767\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      i: 0.5477\n",
      "      a: 0.4902\n",
      "      e: 0.4441\n",
      "      d: 0.3163\n",
      "      l: 0.3060\n",
      "  -> Agent's Final Choice: 'i'\n",
      "  Wrong! Mistakes: 2/6\n",
      "--- Step 4 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      r: 0.1469\n",
      "      d: 0.0947\n",
      "      t: 0.0859\n",
      "      n: 0.0837\n",
      "      o: 0.0813\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      d: 0.3243\n",
      "      l: 0.3122\n",
      "  -> Agent's Final Choice: 'd'\n",
      "  Wrong! Mistakes: 3/6\n",
      "--- Step 5 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      r: 0.1623\n",
      "      t: 0.0949\n",
      "      n: 0.0925\n",
      "      o: 0.0898\n",
      "      l: 0.0809\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      r: 0.3237\n",
      "      l: 0.3199\n",
      "  -> Agent's Final Choice: 'r'\n",
      "  Correct! Word: _ _ _ _ _ _ e r\n",
      "--- Step 6 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      o: 0.1094\n",
      "      t: 0.1013\n",
      "      n: 0.0859\n",
      "      l: 0.0836\n",
      "      s: 0.0789\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      l: 0.3226\n",
      "      o: 0.3017\n",
      "  -> Agent's Final Choice: 'l'\n",
      "  Wrong! Mistakes: 4/6\n",
      "--- Step 7 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      o: 0.1193\n",
      "      t: 0.1105\n",
      "      n: 0.0938\n",
      "      s: 0.0861\n",
      "      c: 0.0727\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      o: 0.3116\n",
      "      t: 0.3087\n",
      "  -> Agent's Final Choice: 'o'\n",
      "  Correct! Word: _ o _ _ _ _ e r\n",
      "--- Step 8 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      t: 0.1226\n",
      "      n: 0.1101\n",
      "      s: 0.0969\n",
      "      c: 0.0867\n",
      "      p: 0.0825\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      t: 0.3208\n",
      "      n: 0.3079\n",
      "  -> Agent's Final Choice: 't'\n",
      "  Correct! Word: _ o _ _ _ t e r\n",
      "--- Step 9 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      n: 0.1283\n",
      "      s: 0.1166\n",
      "      c: 0.1088\n",
      "      p: 0.1010\n",
      "      u: 0.0978\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      n: 0.3261\n",
      "      m: 0.3096\n",
      "  -> Agent's Final Choice: 'n'\n",
      "  Wrong! Mistakes: 5/6\n",
      "--- Step 10 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      s: 0.1338\n",
      "      c: 0.1248\n",
      "      p: 0.1158\n",
      "      u: 0.1122\n",
      "      m: 0.1112\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      c: 0.3242\n",
      "      m: 0.3239\n",
      "  -> Agent's Final Choice: 'c'\n",
      "  Correct! Word: c o _ _ _ t e r\n",
      "--- Step 11 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      m: 0.1875\n",
      "      u: 0.1541\n",
      "      p: 0.1348\n",
      "      s: 0.1253\n",
      "      b: 0.0655\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      m: 0.4002\n",
      "      u: 0.3571\n",
      "  -> Agent's Final Choice: 'm'\n",
      "  Correct! Word: c o m _ _ t e r\n",
      "--- Step 12 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      p: 0.2316\n",
      "      b: 0.1964\n",
      "      u: 0.1679\n",
      "      s: 0.1012\n",
      "      h: 0.0672\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      p: 0.4364\n",
      "      b: 0.3886\n",
      "  -> Agent's Final Choice: 'p'\n",
      "  Correct! Word: c o m p _ t e r\n",
      "--- Step 13 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      h: 0.2291\n",
      "      u: 0.1951\n",
      "      s: 0.1045\n",
      "      y: 0.0790\n",
      "      b: 0.0658\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      h: 0.4660\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      u: 0.3981\n",
      "  -> Agent's Final Choice: 'h'\n",
      "  Wrong! Mistakes: 6/6\n",
      " Lost! Word was: computer\n",
      "\n",
      "Playing with word: algorithm (RL mode - Epsilon=0 for Demo)\n",
      "\n",
      "Target word: algorithm\n",
      "Starting game: _ _ _ _ _ _ _ _ _\n",
      "--- Step 1 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      e: 0.1089\n",
      "      i: 0.0888\n",
      "      a: 0.0871\n",
      "      r: 0.0714\n",
      "      o: 0.0711\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.5773\n",
      "      e: 0.5530\n",
      "      i: 0.5520\n",
      "      l: 0.2972\n",
      "      v: 0.2762\n",
      "  -> Agent's Final Choice: 'a'\n",
      "  Correct! Word: a _ _ _ _ _ _ _ _\n",
      "--- Step 2 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      e: 0.1150\n",
      "      i: 0.0958\n",
      "      n: 0.0950\n",
      "      r: 0.0836\n",
      "      o: 0.0749\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      e: 0.5590\n",
      "      i: 0.5590\n",
      "      a: 0.4902\n",
      "      l: 0.3102\n",
      "      n: 0.2928\n",
      "  -> Agent's Final Choice: 'e'\n",
      "  Wrong! Mistakes: 1/6\n",
      "--- Step 3 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      i: 0.1083\n",
      "      n: 0.1074\n",
      "      r: 0.0945\n",
      "      o: 0.0847\n",
      "      t: 0.0834\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      i: 0.5714\n",
      "      a: 0.4902\n",
      "      e: 0.4441\n",
      "      l: 0.3194\n",
      "      n: 0.3052\n",
      "  -> Agent's Final Choice: 'i'\n",
      "  Correct! Word: a _ _ _ _ i _ _ _\n",
      "--- Step 4 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      n: 0.1205\n",
      "      r: 0.0982\n",
      "      t: 0.0956\n",
      "      o: 0.0900\n",
      "      l: 0.0841\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      l: 0.3230\n",
      "      n: 0.3183\n",
      "  -> Agent's Final Choice: 'l'\n",
      "  Correct! Word: a l _ _ _ i _ _ _\n",
      "--- Step 5 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      o: 0.1174\n",
      "      t: 0.1149\n",
      "      n: 0.0952\n",
      "      s: 0.0835\n",
      "      r: 0.0799\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      t: 0.3130\n",
      "      o: 0.3097\n",
      "  -> Agent's Final Choice: 't'\n",
      "  Correct! Word: a l _ _ _ i t _ _\n",
      "--- Step 6 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      o: 0.1340\n",
      "      r: 0.0968\n",
      "      s: 0.0825\n",
      "      n: 0.0808\n",
      "      u: 0.0691\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      o: 0.3263\n",
      "      v: 0.2954\n",
      "  -> Agent's Final Choice: 'o'\n",
      "  Correct! Word: a l _ o _ i t _ _\n",
      "--- Step 7 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      r: 0.1101\n",
      "      s: 0.0991\n",
      "      n: 0.0975\n",
      "      m: 0.0826\n",
      "      u: 0.0756\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2985\n",
      "      n: 0.2953\n",
      "  -> Agent's Final Choice: 'v'\n",
      "  Wrong! Mistakes: 2/6\n",
      "--- Step 8 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      r: 0.1136\n",
      "      s: 0.1023\n",
      "      n: 0.1006\n",
      "      m: 0.0852\n",
      "      u: 0.0780\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      n: 0.2984\n",
      "      m: 0.2979\n",
      "  -> Agent's Final Choice: 'n'\n",
      "  Wrong! Mistakes: 3/6\n",
      "--- Step 9 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      r: 0.1264\n",
      "      s: 0.1137\n",
      "      m: 0.0948\n",
      "      u: 0.0868\n",
      "      c: 0.0807\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      m: 0.3074\n",
      "      d: 0.2979\n",
      "  -> Agent's Final Choice: 'm'\n",
      "  Correct! Word: a l _ o _ i t _ m\n",
      "--- Step 10 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      r: 0.1392\n",
      "      s: 0.1176\n",
      "      u: 0.1069\n",
      "      c: 0.0892\n",
      "      p: 0.0812\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      u: 0.3099\n",
      "      r: 0.3006\n",
      "  -> Agent's Final Choice: 'u'\n",
      "  Wrong! Mistakes: 4/6\n",
      "--- Step 11 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      r: 0.1559\n",
      "      s: 0.1317\n",
      "      c: 0.0999\n",
      "      p: 0.0909\n",
      "      b: 0.0735\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      r: 0.3173\n",
      "      d: 0.2994\n",
      "  -> Agent's Final Choice: 'r'\n",
      "  Correct! Word: a l _ o r i t _ m\n",
      "--- Step 12 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      s: 0.1369\n",
      "      c: 0.1070\n",
      "      b: 0.0870\n",
      "      f: 0.0813\n",
      "      d: 0.0784\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      w: 0.3239\n",
      "      h: 0.3110\n",
      "  -> Agent's Final Choice: 'w'\n",
      "  Wrong! Mistakes: 5/6\n",
      "--- Step 13 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      s: 0.1458\n",
      "      c: 0.1139\n",
      "      b: 0.0926\n",
      "      f: 0.0866\n",
      "      d: 0.0835\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      h: 0.3158\n",
      "      c: 0.3134\n",
      "  -> Agent's Final Choice: 'h'\n",
      "  Correct! Word: a l _ o r i t h m\n",
      "--- Step 14 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      s: 0.1262\n",
      "      b: 0.1145\n",
      "      c: 0.1133\n",
      "      f: 0.1130\n",
      "      d: 0.0949\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      d: 0.3245\n",
      "      c: 0.3127\n",
      "  -> Agent's Final Choice: 'd'\n",
      "  Wrong! Mistakes: 6/6\n",
      " Lost! Word was: algorithm\n",
      "\n",
      "Playing with word: machine (RL mode - Epsilon=0 for Demo)\n",
      "\n",
      "Target word: machine\n",
      "Starting game: _ _ _ _ _ _ _\n",
      "--- Step 1 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      e: 0.1075\n",
      "      a: 0.0956\n",
      "      i: 0.0806\n",
      "      r: 0.0714\n",
      "      o: 0.0657\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.5858\n",
      "      e: 0.5516\n",
      "      i: 0.5437\n",
      "      l: 0.2980\n",
      "      v: 0.2759\n",
      "  -> Agent's Final Choice: 'a'\n",
      "  Correct! Word: _ a _ _ _ _ _\n",
      "--- Step 2 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      e: 0.0962\n",
      "      r: 0.0864\n",
      "      i: 0.0803\n",
      "      n: 0.0772\n",
      "      t: 0.0751\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      i: 0.5434\n",
      "      e: 0.5403\n",
      "      a: 0.4902\n",
      "      l: 0.3105\n",
      "      v: 0.2779\n",
      "  -> Agent's Final Choice: 'i'\n",
      "  Correct! Word: _ a _ _ i _ _\n",
      "--- Step 3 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      n: 0.1152\n",
      "      s: 0.0958\n",
      "      e: 0.0928\n",
      "      r: 0.0854\n",
      "      t: 0.0833\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      e: 0.5369\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      l: 0.3147\n",
      "      n: 0.3130\n",
      "  -> Agent's Final Choice: 'e'\n",
      "  Correct! Word: _ a _ _ i _ e\n",
      "--- Step 4 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      n: 0.1275\n",
      "      s: 0.1055\n",
      "      t: 0.0976\n",
      "      r: 0.0943\n",
      "      l: 0.0862\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      n: 0.3253\n",
      "      l: 0.3252\n",
      "  -> Agent's Final Choice: 'n'\n",
      "  Correct! Word: _ a _ _ i n e\n",
      "--- Step 5 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      r: 0.1104\n",
      "      t: 0.0870\n",
      "      s: 0.0868\n",
      "      c: 0.0834\n",
      "      l: 0.0817\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      l: 0.3207\n",
      "      w: 0.2875\n",
      "  -> Agent's Final Choice: 'l'\n",
      "  Wrong! Mistakes: 1/6\n",
      "--- Step 6 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      r: 0.1202\n",
      "      t: 0.0947\n",
      "      s: 0.0945\n",
      "      c: 0.0908\n",
      "      m: 0.0738\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      t: 0.2929\n",
      "      c: 0.2903\n",
      "  -> Agent's Final Choice: 't'\n",
      "  Wrong! Mistakes: 2/6\n",
      "--- Step 7 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      r: 0.1328\n",
      "      s: 0.1044\n",
      "      c: 0.1003\n",
      "      m: 0.0816\n",
      "      p: 0.0681\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      c: 0.2998\n",
      "      m: 0.2942\n",
      "  -> Agent's Final Choice: 'c'\n",
      "  Correct! Word: _ a c _ i n e\n",
      "--- Step 8 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      h: 0.1781\n",
      "      r: 0.1159\n",
      "      m: 0.1014\n",
      "      k: 0.0965\n",
      "      o: 0.0884\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      h: 0.4150\n",
      "      m: 0.3141\n",
      "  -> Agent's Final Choice: 'h'\n",
      "  Correct! Word: _ a c h i n e\n",
      "--- Step 9 (Epsilon: 0.000) ---\n",
      "  -> Decision: EXPLOITATION (Hybrid Q-Value)\n",
      "    HMM Probabilities (Baseline):\n",
      "      m: 0.1712\n",
      "      s: 0.1100\n",
      "      r: 0.0837\n",
      "      f: 0.0802\n",
      "      b: 0.0799\n",
      "    DQN Advantage (Correction):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      v: 0.2671\n",
      "      w: 0.2627\n",
      "    Hybrid Q-Value (Final):\n",
      "      a: 0.4902\n",
      "      i: 0.4631\n",
      "      e: 0.4441\n",
      "      m: 0.3838\n",
      "      v: 0.2953\n",
      "  -> Agent's Final Choice: 'm'\n",
      "  Correct! Word: m a c h i n e\n",
      "Won in 9 steps!\n",
      "\n",
      "============================================================\n",
      "COMPREHENSIVE EVALUATION\n",
      "============================================================\n",
      "Loaded 2000 test words\n",
      "\n",
      "RL-Hybrid Evaluation (Epsilon=0.0):\n",
      "\n",
      "Evaluating RL-Player (Epsilon=0) on 2000 games...\n",
      "Played 1/2000 games...\n",
      "Played 100/2000 games...\n",
      "Played 200/2000 games...\n",
      "Played 300/2000 games...\n",
      "Played 400/2000 games...\n",
      "Played 500/2000 games...\n",
      "Played 600/2000 games...\n",
      "Played 700/2000 games...\n",
      "Played 800/2000 games...\n",
      "Played 900/2000 games...\n",
      "Played 1000/2000 games...\n",
      "Played 1100/2000 games...\n",
      "Played 1200/2000 games...\n",
      "Played 1300/2000 games...\n",
      "Played 1400/2000 games...\n",
      "Played 1500/2000 games...\n",
      "Played 1600/2000 games...\n",
      "Played 1700/2000 games...\n",
      "Played 1800/2000 games...\n",
      "Played 1900/2000 games...\n",
      "Played 2000/2000 games...\n",
      "\n",
      "RL-Player Results:\n",
      "Success Rate: 0.281 (563/2000)\n",
      "Average Steps per Game: 11.39\n",
      "Average Wrong Guesses: 5.36\n",
      "Average Repeated Guesses: 0.00\n",
      "Final Score: -53002.00\n",
      "Restored RL Agent epsilon to 0.010\n",
      "\n",
      "============================================================\n",
      "FINAL RL-HYBRID AGENT SCORE\n",
      "============================================================\n",
      "Success Rate              0.281     \n",
      "Avg Wrong Guesses         5.36      \n",
      "Avg Repeated Guesses      0.00      \n",
      "Final Score               -53002.00 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class AdvancedHMMHangman:\n",
    "    def __init__(self, corpus_file):\n",
    "        self.corpus_file = corpus_file\n",
    "        self.words = []\n",
    "        self.hmm_models = {}\n",
    "        self.trigram_models = {}\n",
    "        self.alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        self.vocab_size = len(self.alphabet)\n",
    "        self.letter_freq = None\n",
    "        self.bigram_probs = None\n",
    "        self.trigram_probs = None\n",
    "\n",
    "    def load_corpus(self):\n",
    "        \"\"\"Load and preprocess the corpus\"\"\"\n",
    "        print(\"Loading corpus...\")\n",
    "        try:\n",
    "            with open(self.corpus_file, 'r') as f:\n",
    "                self.words = [word.strip().lower() for word in f.readlines()]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Corpus file '{self.corpus_file}' not found.\")\n",
    "            # Try to load from a default path if it exists, or raise error\n",
    "            # For this example, we'll assume it's in the same directory.\n",
    "            # In a real scenario, you might have a fallback or user input.\n",
    "            return\n",
    "\n",
    "        self.words = [word for word in self.words if re.match('^[a-z]+$', word)]\n",
    "        print(f\"Loaded {len(self.words)} valid words\")\n",
    "        all_letters = ''.join(self.words)\n",
    "        self.letter_freq = Counter(all_letters)\n",
    "        total_letters = len(all_letters)\n",
    "        self.letter_probs = {char: count/total_letters for char, count in self.letter_freq.items()}\n",
    "\n",
    "        self._calculate_bigram_probs()\n",
    "        self._calculate_trigram_probs()\n",
    "\n",
    "    def _calculate_bigram_probs(self):\n",
    "        \"\"\"Calculate bigram probabilities across all words\"\"\"\n",
    "        print(\"Calculating bigram probabilities...\")\n",
    "        bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        for word in self.words:\n",
    "            for i in range(len(word) - 1):\n",
    "                current_char = word[i]\n",
    "                next_char = word[i + 1]\n",
    "                bigram_counts[current_char][next_char] += 1\n",
    "\n",
    "        self.bigram_probs = defaultdict(dict)\n",
    "        for char1 in bigram_counts:\n",
    "            total = sum(bigram_counts[char1].values())\n",
    "            for char2 in bigram_counts[char1]:\n",
    "                self.bigram_probs[char1][char2] = bigram_counts[char1][char2] / total\n",
    "\n",
    "        print(\"Bigram probabilities calculated\")\n",
    "\n",
    "    def _calculate_trigram_probs(self):\n",
    "        \"\"\"Calculate trigram probabilities across all words\"\"\"\n",
    "        print(\"Calculating trigram probabilities...\")\n",
    "        trigram_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "        for word in self.words:\n",
    "            for i in range(len(word) - 2):\n",
    "                char1 = word[i]\n",
    "                char2 = word[i + 1]\n",
    "                char3 = word[i + 2]\n",
    "                trigram_counts[char1][char2][char3] += 1\n",
    "\n",
    "        self.trigram_probs = defaultdict(lambda: defaultdict(dict))\n",
    "        for char1 in trigram_counts:\n",
    "            for char2 in trigram_counts[char1]:\n",
    "                total = sum(trigram_counts[char1][char2].values())\n",
    "                for char3 in trigram_counts[char1][char2]:\n",
    "                    self.trigram_probs[char1][char2][char3] = trigram_counts[char1][char2][char3] / total if total > 0 else 0\n",
    "\n",
    "        print(\"Trigram probabilities calculated\")\n",
    "\n",
    "    def train_hmm(self):\n",
    "        \"\"\"Train enhanced HMM with trigram features\"\"\"\n",
    "        print(\"\\nTraining Enhanced HMM models with trigrams...\")\n",
    "\n",
    "        words_by_length = defaultdict(list)\n",
    "        for word in self.words:\n",
    "            words_by_length[len(word)].append(word)\n",
    "\n",
    "        for length, word_list in words_by_length.items():\n",
    "            if len(word_list) < 5:\n",
    "                continue\n",
    "\n",
    "            print(f\"Training HMM for word length {length} with {len(word_list)} words\")\n",
    "\n",
    "            # Enhanced transition matrices including trigrams\n",
    "            transition = np.ones((length, self.vocab_size, self.vocab_size)) * 1e-10\n",
    "            trigram_transition = np.ones((length, self.vocab_size, self.vocab_size, self.vocab_size)) * 1e-10\n",
    "            emission = np.ones((length, self.vocab_size)) * 1e-10\n",
    "            position_freq = np.ones((length, self.vocab_size)) * 1e-10\n",
    "\n",
    "            for word in word_list:\n",
    "                indices = [ord(c) - ord('a') for c in word]\n",
    "\n",
    "                for pos, char_idx in enumerate(indices):\n",
    "                    emission[pos, char_idx] += 1\n",
    "                    position_freq[pos, char_idx] += 1\n",
    "\n",
    "                # Bigram transitions\n",
    "                for i in range(len(indices) - 1):\n",
    "                    current_char = indices[i]\n",
    "                    next_char = indices[i + 1]\n",
    "                    transition[i, current_char, next_char] += 1\n",
    "\n",
    "                # Trigram transitions\n",
    "                for i in range(len(indices) - 2):\n",
    "                    char1 = indices[i]\n",
    "                    char2 = indices[i + 1]\n",
    "                    char3 = indices[i + 2]\n",
    "                    trigram_transition[i, char1, char2, char3] += 1\n",
    "\n",
    "            # Normalization\n",
    "            for pos in range(length):\n",
    "                if emission[pos].sum() > 0:\n",
    "                    emission[pos] /= emission[pos].sum()\n",
    "                if position_freq[pos].sum() > 0:\n",
    "                    position_freq[pos] /= position_freq[pos].sum()\n",
    "                for i in range(self.vocab_size):\n",
    "                    if transition[pos, i].sum() > 0:\n",
    "                        transition[pos, i] /= transition[pos, i].sum()\n",
    "                    for j in range(self.vocab_size):\n",
    "                        if trigram_transition[pos, i, j].sum() > 0:\n",
    "                            trigram_transition[pos, i, j] /= trigram_transition[pos, i, j].sum()\n",
    "\n",
    "            self.hmm_models[length] = {\n",
    "                'transition': transition,\n",
    "                'trigram_transition': trigram_transition,\n",
    "                'emission': emission,\n",
    "                'position_freq': position_freq,\n",
    "                'initial': self._compute_initial_probs(word_list),\n",
    "                'word_count': len(word_list)\n",
    "            }\n",
    "\n",
    "        print(f\"Trained HMMs for {len(self.hmm_models)} different word lengths\")\n",
    "\n",
    "    def _compute_initial_probs(self, word_list):\n",
    "        \"\"\"Compute initial letter probabilities\"\"\"\n",
    "        initial_counts = np.zeros(self.vocab_size)\n",
    "        for word in word_list:\n",
    "            if word:\n",
    "                initial_counts[ord(word[0]) - ord('a')] += 1\n",
    "        if initial_counts.sum() > 0:\n",
    "            initial_counts /= initial_counts.sum()\n",
    "        return initial_counts\n",
    "\n",
    "    def get_letter_probabilities(self, masked_word, guessed_letters):\n",
    "        \"\"\"\n",
    "        Enhanced probability calculation with trigram features\n",
    "        \"\"\"\n",
    "        length = len(masked_word)\n",
    "\n",
    "        # If no HMM for this length, use fallback\n",
    "        if length not in self.hmm_models:\n",
    "            return self._smart_fallback(guessed_letters)\n",
    "\n",
    "        model = self.hmm_models[length]\n",
    "        probs = np.zeros(self.vocab_size)\n",
    "        position_scores = np.zeros(self.vocab_size)\n",
    "\n",
    "        # Enhanced pattern analysis with trigrams\n",
    "        for pos, char in enumerate(masked_word):\n",
    "            if char == '_':\n",
    "                # Emission probability\n",
    "                position_scores += model['emission'][pos]\n",
    "\n",
    "                # Bigram context\n",
    "                if pos > 0 and masked_word[pos-1] != '_':\n",
    "                    prev_char_idx = ord(masked_word[pos-1]) - ord('a')\n",
    "                    position_scores += model['transition'][pos-1, prev_char_idx]\n",
    "\n",
    "                # Trigram context\n",
    "                if pos > 1 and masked_word[pos-2] != '_' and masked_word[pos-1] != '_':\n",
    "                    char1_idx = ord(masked_word[pos-2]) - ord('a')\n",
    "                    char2_idx = ord(masked_word[pos-1]) - ord('a')\n",
    "                    position_scores += model['trigram_transition'][pos-2, char1_idx, char2_idx]\n",
    "\n",
    "        pattern_scores = self._pattern_matching(masked_word, guessed_letters, length)\n",
    "\n",
    "        freq_scores = np.array([self.letter_probs.get(chr(i + ord('a')), 0)\n",
    "                               for i in range(self.vocab_size)])\n",
    "\n",
    "        # Weighted combination of features\n",
    "        probs = (0.25 * position_scores +\n",
    "                 0.35 * pattern_scores +\n",
    "                 0.20 * freq_scores +\n",
    "                 0.20 * self._trigram_context_score(masked_word))\n",
    "\n",
    "        probs = self._apply_guessed_letters_filter(probs, guessed_letters)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def _trigram_context_score(self, masked_word):\n",
    "        \"\"\"Calculate trigram-based context scores\"\"\"\n",
    "        scores = np.zeros(self.vocab_size)\n",
    "        length = len(masked_word)\n",
    "\n",
    "        for i in range(length):\n",
    "            if masked_word[i] == '_':\n",
    "                # Look for trigram patterns\n",
    "                if i >= 2 and masked_word[i-2] != '_' and masked_word[i-1] != '_':\n",
    "                    char1 = masked_word[i-2]\n",
    "                    char2 = masked_word[i-1]\n",
    "                    for char3_idx in range(self.vocab_size):\n",
    "                        char3 = chr(char3_idx + ord('a'))\n",
    "                        if char1 in self.trigram_probs and char2 in self.trigram_probs[char1]:\n",
    "                            scores[char3_idx] += self.trigram_probs[char1][char2].get(char3, 0)\n",
    "\n",
    "                # Also look ahead\n",
    "                if i < length - 2 and masked_word[i+1] != '_' and masked_word[i+2] != '_':\n",
    "                    char2 = masked_word[i+1]\n",
    "                    char3 = masked_word[i+2]\n",
    "                    for char1_idx in range(self.vocab_size):\n",
    "                        char1 = chr(char1_idx + ord('a'))\n",
    "                        if char1 in self.trigram_probs and char2 in self.trigram_probs[char1]:\n",
    "                            scores[char1_idx] += self.trigram_probs[char1][char2].get(char3, 0)\n",
    "\n",
    "        if scores.sum() > 0:\n",
    "            scores /= scores.sum()\n",
    "        return scores\n",
    "\n",
    "    def _pattern_matching(self, masked_word, guessed_letters, length):\n",
    "        \"\"\"Enhanced pattern matching with trigram consideration\"\"\"\n",
    "        if length not in self.hmm_models:\n",
    "            return np.ones(self.vocab_size) / self.vocab_size\n",
    "\n",
    "        # Regex pattern\n",
    "        pattern = masked_word.replace('_', '[a-z]')\n",
    "        matching_words = [word for word in self.words\n",
    "                         if len(word) == length and re.match(pattern, word)]\n",
    "\n",
    "        if not matching_words:\n",
    "            return np.ones(self.vocab_size) / self.vocab_size\n",
    "\n",
    "        letter_counts = Counter()\n",
    "        for word in matching_words:\n",
    "            letter_counts.update(word)\n",
    "\n",
    "        total = sum(letter_counts.values())\n",
    "        pattern_scores = np.zeros(self.vocab_size)\n",
    "        for i in range(self.vocab_size):\n",
    "            letter = chr(i + ord('a'))\n",
    "            pattern_scores[i] = letter_counts[letter] / total if total > 0 else 0\n",
    "\n",
    "        return pattern_scores\n",
    "\n",
    "    def _smart_fallback(self, guessed_letters):\n",
    "        \"\"\"Smart fallback using letter frequency and vowel/consonant patterns\"\"\"\n",
    "        probs = np.array([self.letter_probs.get(chr(i + ord('a')), 0)\n",
    "                         for i in range(self.vocab_size)])\n",
    "\n",
    "        vowels = {'a', 'e', 'i', 'o', 'u'}\n",
    "        guessed_vowels = len([l for l in guessed_letters if l in vowels])\n",
    "        if guessed_vowels < 2:\n",
    "            for i, letter in enumerate(self.alphabet):\n",
    "                if letter in vowels:\n",
    "                    probs[i] *= 2\n",
    "\n",
    "        return self._apply_guessed_letters_filter(probs, guessed_letters)\n",
    "\n",
    "    def _apply_guessed_letters_filter(self, probs, guessed_letters):\n",
    "        \"\"\"Set probability to 0 for already guessed letters and normalize\"\"\"\n",
    "        for letter in guessed_letters:\n",
    "            if letter in self.alphabet:\n",
    "                idx = ord(letter) - ord('a')\n",
    "                probs[idx] = 0\n",
    "\n",
    "        if probs.sum() > 0:\n",
    "            probs /= probs.sum()\n",
    "        else:\n",
    "            probs = np.ones(self.vocab_size)\n",
    "            for letter in guessed_letters:\n",
    "                if letter in self.alphabet:\n",
    "                    idx = ord(letter) - ord('a')\n",
    "                    probs[idx] = 0\n",
    "            if probs.sum() > 0:\n",
    "                probs /= probs.sum()\n",
    "            else:\n",
    "                probs = np.ones(self.vocab_size) / self.vocab_size\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def analyze_corpus(self):\n",
    "        \"\"\"Comprehensive corpus analysis\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"CORPUS ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Letter frequency\n",
    "        print(\"\\nTop 15 most common letters:\")\n",
    "        if self.letter_freq:\n",
    "            for letter, count in self.letter_freq.most_common(15):\n",
    "                print(f\"{letter}: {count} ({count/len(''.join(self.words))*100:.2f}%)\")\n",
    "        else:\n",
    "            print(\"Corpus not loaded, no frequencies to show.\")\n",
    "\n",
    "        # Word length distribution\n",
    "        if self.words:\n",
    "            word_lengths = [len(word) for word in self.words]\n",
    "            print(f\"\\nWord length statistics:\")\n",
    "            print(f\"Min: {min(word_lengths)}, Max: {max(word_lengths)}, Avg: {np.mean(word_lengths):.2f}\")\n",
    "        else:\n",
    "            print(\"\\nCorpus not loaded, no word length stats.\")\n",
    "\n",
    "        # HMM coverage\n",
    "        print(f\"\\nHMM coverage: {len(self.hmm_models)} different word lengths\")\n",
    "        for length in sorted(self.hmm_models.keys()):\n",
    "            count = self.hmm_models[length]['word_count']\n",
    "            print(f\"Length {length}: {count} words\")\n",
    "\n",
    "        return self.letter_freq\n",
    "\n",
    "class HangmanEnvironment:\n",
    "    \"\"\"Hangman game environment for RL\"\"\"\n",
    "    def __init__(self, word_list, max_wrong=6):\n",
    "        self.word_list = word_list\n",
    "        self.max_wrong = max_wrong\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment\"\"\"\n",
    "        self.target_word = random.choice(self.word_list)\n",
    "        self.masked_word = ['_'] * len(self.target_word)\n",
    "        self.guessed_letters = set()\n",
    "        self.wrong_guesses = 0\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"Get current state representation (ENVIRONMENT STATE ONLY)\"\"\"\n",
    "        # Convert masked word to numerical representation\n",
    "        masked_vec = [ord(c) - ord('a') if c != '_' else 26 for c in self.masked_word]\n",
    "\n",
    "        # Convert guessed letters to binary vector\n",
    "        guessed_vec = [1 if chr(i + ord('a')) in self.guessed_letters else 0\n",
    "                      for i in range(26)]\n",
    "\n",
    "        # Normalize lengths for neural network\n",
    "        max_length = 15\n",
    "        if len(masked_vec) < max_length:\n",
    "            masked_vec += [27] * (max_length - len(masked_vec))\n",
    "        else:\n",
    "            masked_vec = masked_vec[:max_length]\n",
    "\n",
    "        state = masked_vec + guessed_vec + [self.wrong_guesses / self.max_wrong]\n",
    "        return np.array(state, dtype=np.float32)\n",
    "\n",
    "    def step(self, action, hmm_model):\n",
    "        \"\"\"Take an action (guess a letter)\"\"\"\n",
    "        letter = chr(action + ord('a'))\n",
    "        reward = 0\n",
    "\n",
    "        if letter in self.guessed_letters:\n",
    "            # Repeated guess penalty\n",
    "            reward = -2\n",
    "            self.reward = reward\n",
    "            return self._get_state(), reward, self.done, {\"reason\": \"repeated\"}\n",
    "\n",
    "        self.guessed_letters.add(letter)\n",
    "\n",
    "        if letter in self.target_word:\n",
    "            # Correct guess\n",
    "            reward = 1\n",
    "            for i, char in enumerate(self.target_word):\n",
    "                if char == letter:\n",
    "                    self.masked_word[i] = letter\n",
    "        else:\n",
    "            # Wrong guess\n",
    "            self.wrong_guesses += 1\n",
    "            reward = -1\n",
    "\n",
    "        # Check if game is over\n",
    "        if '_' not in self.masked_word:\n",
    "            self.done = True\n",
    "            reward = 10  # Winning bonus\n",
    "        elif self.wrong_guesses >= self.max_wrong:\n",
    "            self.done = True\n",
    "            reward = -5  # Losing penalty\n",
    "\n",
    "        self.reward = reward\n",
    "        return self._get_state(), reward, self.done, {}\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network for Hangman - This network now learns the ADVANTAGE\"\"\"\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        # Output is the \"Advantage\" (A(s,a))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class RLHangmanAgent:\n",
    "    \"\"\"RL Agent for Hangman - Now uses a truly hybrid Advantage-Learning model\"\"\"\n",
    "    def __init__(self, state_size, action_size, hmm_model, lr=0.001, gamma=0.99):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.hmm_model = hmm_model\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9994 # Slower decay\n",
    "\n",
    "        self.batch_size = 32\n",
    "        self.memory = deque(maxlen=20000) # Increased memory\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"RL Agent using device: {self.device}\")\n",
    "\n",
    "        # The DQN learns the \"Advantage\" A(s,a)\n",
    "        self.model = DQN(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in memory\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, masked_word, guessed_letters, verbose=False):\n",
    "        \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "\n",
    "        def print_top_k(probs, k=5, title=\"\"):\n",
    "            print(f\"    {title}:\")\n",
    "            indices = np.argsort(probs)[-k:][::-1]\n",
    "            for i in indices:\n",
    "                if probs[i] > -np.inf and probs[i] > 0.0001:\n",
    "                    print(f\"      {chr(i + ord('a'))}: {probs[i]:.4f}\")\n",
    "\n",
    "        # Exploration: Use HMM's probabilities to pick an action\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            probs = self.hmm_model.get_letter_probabilities(masked_word, guessed_letters)\n",
    "            if verbose:\n",
    "                print(\"  -> Decision: EXPLORATION (HMM-guided random choice)\")\n",
    "                print_top_k(probs, k=5, title=\"HMM Probabilities\")\n",
    "\n",
    "            if probs.sum() > 0:\n",
    "                return np.random.choice(self.action_size, p=probs)\n",
    "            else: # Fallback if all probs are zero\n",
    "                available_actions = [i for i in range(self.action_size) if chr(i + ord('a')) not in guessed_letters]\n",
    "                return random.choice(available_actions) if available_actions else 0\n",
    "\n",
    "        # Exploitation: Use Hybrid Q-Value\n",
    "        else:\n",
    "            # 1. Get HMM Probabilities (The \"Baseline\")\n",
    "            # We use state[-26:] which is the hmm_probs we stored in the state vector\n",
    "            hmm_probs = state[-26:]\n",
    "\n",
    "            # 2. Get DQN Advantage (The \"Correction\")\n",
    "            state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                advantage_values = self.model(state_tensor).cpu().data.numpy()[0]\n",
    "            self.model.train()\n",
    "\n",
    "            # 3. Create the Hybrid Q-Value\n",
    "            # Q(s,a) = P(a|s) + A(s,a)\n",
    "            hybrid_q_values = hmm_probs + advantage_values\n",
    "\n",
    "            if verbose:\n",
    "                print(\"  -> Decision: EXPLOITATION (Hybrid Q-Value)\")\n",
    "                print_top_k(hmm_probs, k=5, title=\"HMM Probabilities (Baseline)\")\n",
    "                print_top_k(advantage_values, k=5, title=\"DQN Advantage (Correction)\")\n",
    "                print_top_k(hybrid_q_values, k=5, title=\"Hybrid Q-Value (Final)\")\n",
    "\n",
    "            # 4. Mask and choose the best action\n",
    "            for i in range(self.action_size):\n",
    "                if chr(i + ord('a')) in guessed_letters:\n",
    "                    hybrid_q_values[i] = -np.inf\n",
    "\n",
    "            return np.argmax(hybrid_q_values)\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"Train the model on remembered experiences\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.array([e[0] for e in batch])).float().to(self.device)\n",
    "        actions = torch.LongTensor([e[1] for e in batch]).to(self.device)\n",
    "        rewards = torch.FloatTensor([e[2] for e in batch]).to(self.device)\n",
    "        next_states = torch.from_numpy(np.array([e[3] for e in batch])).float().to(self.device)\n",
    "        dones = torch.BoolTensor([e[4] for e in batch]).to(self.device)\n",
    "\n",
    "        # --- Get Q-values for CURRENT state ---\n",
    "        # 1. Get Advantage A(s,a) from the model\n",
    "        current_advantage = self.model(states)\n",
    "        # 2. Get HMM Probs P(a|s) from the state vector\n",
    "        current_hmm_probs = states[:, -26:]\n",
    "        # 3. Q(s,a) = P(a|s) + A(s,a)\n",
    "        current_hybrid_q = current_hmm_probs + current_advantage\n",
    "        # 4. Get the Q-value for the action that was actually taken\n",
    "        current_q = current_hybrid_q.gather(1, actions.unsqueeze(1))\n",
    "\n",
    "        # --- Get Q-values for NEXT state (for Bellman update) ---\n",
    "        # 1. Get Advantage A(s',a) from the model\n",
    "        next_advantage = self.model(next_states).detach()\n",
    "        # 2. Get HMM Probs P(a|s') from the next_state vector\n",
    "        next_hmm_probs = next_states[:, -26:]\n",
    "        # 3. Q(s',a) = P(a|s') + A(s',a)\n",
    "        next_hybrid_q = next_hmm_probs + next_advantage\n",
    "        # 4. Find the max Q-value for the next state\n",
    "        next_q = next_hybrid_q.max(1)[0]\n",
    "\n",
    "        # --- Calculate Target and Loss ---\n",
    "        target_q = rewards + (self.gamma * next_q * ~dones)\n",
    "\n",
    "        loss = self.criterion(current_q.squeeze(), target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "class HybridHangmanPlayer:\n",
    "    \"\"\"Hybrid player combining HMM and RL\"\"\"\n",
    "    def __init__(self, hmm_model, use_rl=True):\n",
    "        self.hmm_model = hmm_model\n",
    "        self.use_rl = use_rl\n",
    "\n",
    "        if use_rl:\n",
    "            # **HYBRID STATE FIX**: State size = 15 (masked) + 26 (guessed) + 1 (wrong) + 26 (HMM probs)\n",
    "            state_size = 68\n",
    "            action_size = 26\n",
    "            self.rl_agent = RLHangmanAgent(state_size, action_size, hmm_model)\n",
    "            self.env = HangmanEnvironment(hmm_model.words)\n",
    "\n",
    "        self.games_played = 0\n",
    "        self.games_won = 0\n",
    "        self.total_wrong_guesses = 0\n",
    "        self.total_repeated_guesses = 0\n",
    "\n",
    "    def train_rl(self, episodes=1000):\n",
    "        \"\"\"Train the RL agent\"\"\"\n",
    "        if not self.use_rl or not hasattr(self, 'rl_agent'):\n",
    "            print(\"RL agent not initialized. Skipping training.\")\n",
    "            return []\n",
    "\n",
    "        print(f\"Training RL agent for {episodes} episodes...\")\n",
    "        rewards = []\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            env_state = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                # Get current game state for HMM\n",
    "                masked_word = ''.join(self.env.masked_word)\n",
    "                guessed_letters = self.env.guessed_letters\n",
    "\n",
    "                # **HYBRID STATE FIX**: Create the FULL hybrid state\n",
    "                hmm_probs = self.hmm_model.get_letter_probabilities(masked_word, guessed_letters)\n",
    "                full_state = np.concatenate((env_state, hmm_probs))\n",
    "\n",
    "                # Pass verbose=False during training\n",
    "                action = self.rl_agent.act(full_state, masked_word, guessed_letters, verbose=False)\n",
    "                env_next_state, reward, done, info = self.env.step(action, self.hmm_model)\n",
    "\n",
    "                if info.get(\"reason\") == \"repeated\":\n",
    "                    self.total_repeated_guesses += 1\n",
    "\n",
    "                # **HYBRID STATE FIX**: Create the FULL next hybrid state\n",
    "                next_masked_word = ''.join(self.env.masked_word)\n",
    "                next_guessed_letters = self.env.guessed_letters\n",
    "                next_hmm_probs = self.hmm_model.get_letter_probabilities(next_masked_word, next_guessed_letters)\n",
    "                full_next_state = np.concatenate((env_next_state, next_hmm_probs))\n",
    "\n",
    "                self.rl_agent.remember(full_state, action, reward, full_next_state, done)\n",
    "                env_state = env_next_state # The env state for the next loop\n",
    "                total_reward += reward\n",
    "\n",
    "            self.rl_agent.replay()\n",
    "            rewards.append(total_reward)\n",
    "\n",
    "            if (episode+1) % 100 == 0:\n",
    "                avg_reward = np.mean(rewards[-100:])\n",
    "                print(f\"Episode {episode+1}, Average Reward: {avg_reward:.2f}, Epsilon: {self.rl_agent.epsilon:.3f}\")\n",
    "\n",
    "        return rewards\n",
    "\n",
    "    def play_game(self, target_word, max_wrong=6, verbose=False, use_rl=None):\n",
    "        \"\"\"Play a single game\"\"\"\n",
    "        if use_rl is None:\n",
    "            use_rl = self.use_rl\n",
    "\n",
    "        # Reset internal game state for play_game\n",
    "        masked_word = ['_'] * len(target_word)\n",
    "        guessed_letters = set()\n",
    "        wrong_guesses = 0\n",
    "        repeated_guesses = 0\n",
    "        steps = 0\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nTarget word: {target_word}\")\n",
    "            print(f\"Starting game: {' '.join(masked_word)}\")\n",
    "\n",
    "        while wrong_guesses < max_wrong and '_' in masked_word:\n",
    "            steps += 1\n",
    "            letter = ''\n",
    "\n",
    "            if use_rl and hasattr(self, 'rl_agent'):\n",
    "                if verbose:\n",
    "                    print(f\"--- Step {steps} (Epsilon: {self.rl_agent.epsilon:.3f}) ---\")\n",
    "\n",
    "                # **HYBRID STATE FIX**: Get the full hybrid state\n",
    "                state = self._get_rl_state(masked_word, guessed_letters, wrong_guesses, max_wrong)\n",
    "                # Pass verbose flag to 'act' method for detailed logging\n",
    "                action = self.rl_agent.act(state, ''.join(masked_word), guessed_letters, verbose=verbose)\n",
    "                letter = chr(action + ord('a'))\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"  -> Agent's Final Choice: '{letter}'\")\n",
    "            else:\n",
    "                # Use HMM only\n",
    "                probs = self.hmm_model.get_letter_probabilities(''.join(masked_word), guessed_letters)\n",
    "                best_letter_idx = np.argmax(probs)\n",
    "                letter = chr(best_letter_idx + ord('a'))\n",
    "\n",
    "                if verbose:\n",
    "                    top_choices = []\n",
    "                    for i in np.argsort(probs)[-3:][::-1]:\n",
    "                        if probs[i] > 0:\n",
    "                            top_choices.append(f\"{chr(i + ord('a'))}({probs[i]:.3f})\")\n",
    "                    print(f\"Step {steps}: HMM guessing '{letter}' from {top_choices}\")\n",
    "\n",
    "            if letter in guessed_letters:\n",
    "                repeated_guesses += 1\n",
    "                if verbose:\n",
    "                    print(f\"  Repeated guess! ('{letter}')\")\n",
    "                continue\n",
    "\n",
    "            guessed_letters.add(letter)\n",
    "\n",
    "            if letter in target_word:\n",
    "                # Reveal the letter\n",
    "                for i, char in enumerate(target_word):\n",
    "                    if char == letter:\n",
    "                        masked_word[i] = letter\n",
    "                if verbose:\n",
    "                    print(f\"  Correct! Word: {' '.join(masked_word)}\")\n",
    "            else:\n",
    "                wrong_guesses += 1\n",
    "                if verbose:\n",
    "                    print(f\"  Wrong! Mistakes: {wrong_guesses}/{max_wrong}\")\n",
    "\n",
    "        won = '_' not in masked_word\n",
    "        if verbose:\n",
    "            if won:\n",
    "                print(f\"Won in {steps} steps!\")\n",
    "            else:\n",
    "                print(f\" Lost! Word was: {target_word}\")\n",
    "\n",
    "        # Note: This tracks stats for the *class instance* if you were to reuse it.\n",
    "        # The evaluate() method resets its own counters, which is correct.\n",
    "        self.games_played += 1\n",
    "        if won:\n",
    "            self.games_won += 1\n",
    "        self.total_wrong_guesses += wrong_guesses\n",
    "        self.total_repeated_guesses += repeated_guesses\n",
    "\n",
    "        return won, steps, wrong_guesses, repeated_guesses\n",
    "\n",
    "    def _get_rl_state(self, masked_word, guessed_letters, wrong_guesses, max_wrong):\n",
    "        \"\"\"Get RL state representation (FULL HYBRID STATE)\"\"\"\n",
    "        # --- Get Base Environment State ---\n",
    "        masked_vec = [ord(c) - ord('a') if c != '_' else 26 for c in masked_word]\n",
    "        guessed_vec = [1 if chr(i + ord('a')) in guessed_letters else 0 for i in range(26)]\n",
    "\n",
    "        max_length = 15\n",
    "        if len(masked_vec) < max_length:\n",
    "            masked_vec += [27] * (max_length - len(masked_vec))\n",
    "        else:\n",
    "            masked_vec = masked_vec[:max_length]\n",
    "\n",
    "        env_state = masked_vec + guessed_vec + [wrong_guesses / max_wrong]\n",
    "        env_state_np = np.array(env_state, dtype=np.float32)\n",
    "\n",
    "        # --- Get HMM Probability State ---\n",
    "        hmm_probs = self.hmm_model.get_letter_probabilities(''.join(masked_word), guessed_letters)\n",
    "\n",
    "        # --- Concatenate to create FULL HYBRID STATE ---\n",
    "        full_state = np.concatenate((env_state_np, hmm_probs))\n",
    "        return full_state\n",
    "\n",
    "    def evaluate(self, word_list, num_games=1000, verbose=False, use_rl=None):\n",
    "        \"\"\"Evaluate the player\"\"\"\n",
    "        if use_rl is None:\n",
    "            use_rl = self.use_rl\n",
    "\n",
    "        # Ensure RL agent is in evaluation mode (epsilon=0) for fair comparison\n",
    "        original_epsilon = -1\n",
    "        if use_rl and hasattr(self, 'rl_agent'):\n",
    "            original_epsilon = self.rl_agent.epsilon\n",
    "            self.rl_agent.epsilon = 0.0 # Pure exploitation\n",
    "            print(f\"\\nEvaluating RL-Player (Epsilon=0) on {num_games} games...\")\n",
    "        else:\n",
    "            print(f\"\\nEvaluating HMM-Player on {num_games} games...\")\n",
    "\n",
    "        wins = 0\n",
    "        total_steps = 0\n",
    "        total_wrong_guesses = 0\n",
    "        total_repeated_guesses = 0\n",
    "\n",
    "        test_words = random.sample(word_list, min(num_games, len(word_list)))\n",
    "\n",
    "        for i, word in enumerate(test_words):\n",
    "            if verbose and (i == 0 or (i+1) % 100 == 0):\n",
    "                print(f\"Played {i+1}/{len(test_words)} games...\")\n",
    "\n",
    "            won, steps, wrong_guesses, repeated_guesses = self.play_game(\n",
    "                word, verbose=False, use_rl=use_rl\n",
    "            )\n",
    "\n",
    "            if won:\n",
    "                wins += 1\n",
    "            total_steps += steps\n",
    "            total_wrong_guesses += wrong_guesses\n",
    "            total_repeated_guesses += repeated_guesses\n",
    "\n",
    "        num_games_played = len(test_words)\n",
    "        success_rate = wins / num_games_played\n",
    "        avg_steps = total_steps / num_games_played\n",
    "        avg_wrong = total_wrong_guesses / num_games_played\n",
    "        avg_repeated = total_repeated_guesses / num_games_played\n",
    "\n",
    "        # Calculate final score according to the problem statement\n",
    "        final_score = (success_rate * 2000) - (total_wrong_guesses * 5) - (total_repeated_guesses * 2)\n",
    "\n",
    "        print(f\"\\n{'RL-' if use_rl else 'HMM-'}Player Results:\")\n",
    "        print(f\"Success Rate: {success_rate:.3f} ({wins}/{num_games_played})\")\n",
    "        print(f\"Average Steps per Game: {avg_steps:.2f}\")\n",
    "        print(f\"Average Wrong Guesses: {avg_wrong:.2f}\")\n",
    "        print(f\"Average Repeated Guesses: {avg_repeated:.2f}\")\n",
    "        print(f\"Final Score: {final_score:.2f}\")\n",
    "\n",
    "        # Restore original epsilon if it was changed\n",
    "        if original_epsilon != -1:\n",
    "            self.rl_agent.epsilon = original_epsilon\n",
    "            print(f\"Restored RL Agent epsilon to {self.rl_agent.epsilon:.3f}\")\n",
    "\n",
    "        return success_rate, avg_steps, avg_wrong, avg_repeated, final_score\n",
    "\n",
    "def demo_hybrid_solution():\n",
    "    print(\"HYBRID HMM-RL SOLUTION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Initialize and train HMM\n",
    "    hmm_solver = AdvancedHMMHangman('corpus.txt')\n",
    "    hmm_solver.load_corpus()\n",
    "    if not hmm_solver.words:\n",
    "        print(\"Corpus loading failed. Exiting.\")\n",
    "        return None, None\n",
    "    hmm_solver.train_hmm()\n",
    "    hmm_solver.analyze_corpus()\n",
    "\n",
    "    # Create hybrid player\n",
    "    hybrid_player = HybridHangmanPlayer(hmm_solver, use_rl=True)\n",
    "\n",
    "    # Train RL component\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING RL AGENT\")\n",
    "    print(\"=\"*60)\n",
    "    rewards = hybrid_player.train_rl(episodes=10000)\n",
    "\n",
    "    # Demo games\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED RL-HYBRID DEMO\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    demo_words = ['python', 'hangman', 'computer', 'algorithm', 'machine']\n",
    "    for word in demo_words:\n",
    "        print(f\"\\nPlaying with word: {word} (RL mode - Epsilon=0 for Demo)\")\n",
    "        if hasattr(hybrid_player, 'rl_agent'):\n",
    "            original_epsilon = hybrid_player.rl_agent.epsilon\n",
    "            hybrid_player.rl_agent.epsilon = 0.0 # Force exploitation for demo\n",
    "\n",
    "        hybrid_player.play_game(word, verbose=True, use_rl=True)\n",
    "\n",
    "        if hasattr(hybrid_player, 'rl_agent'):\n",
    "            hybrid_player.rl_agent.epsilon = original_epsilon # Restore\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPREHENSIVE EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    try:\n",
    "        with open('test.txt', 'r') as f:\n",
    "            test_words = [word.strip().lower() for word in f.readlines()]\n",
    "        test_words = [word for word in test_words if re.match('^[a-z]+$', word)]\n",
    "        print(f\"Loaded {len(test_words)} test words\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"test.txt not found. Using training words for evaluation (not recommended).\")\n",
    "        test_words = hmm_solver.words\n",
    "\n",
    "    if not test_words:\n",
    "        print(\"No test words available. Skipping evaluation.\")\n",
    "        return hmm_solver, hybrid_player\n",
    "\n",
    "    # Evaluate RL hybrid\n",
    "    print(\"\\nRL-Hybrid Evaluation (Epsilon=0.0):\")\n",
    "    rl_success, rl_steps, rl_wrong, rl_repeated, rl_score = hybrid_player.evaluate(\n",
    "        test_words, num_games=2000, verbose=True, use_rl=True\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RL-HYBRID AGENT SCORE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Success Rate':<25} {rl_success:<10.3f}\")\n",
    "    print(f\"{'Avg Wrong Guesses':<25} {rl_wrong:<10.2f}\")\n",
    "    print(f\"{'Avg Repeated Guesses':<25} {rl_repeated:<10.2f}\")\n",
    "    print(f\"{'Final Score':<25} {rl_score:<10.2f}\")\n",
    "\n",
    "    return hmm_solver, hybrid_player\n",
    "\n",
    "# Run the complete solution\n",
    "hmm_solver, hybrid_player = demo_hybrid_solution()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPCr+h/N/ytMAriasFgU1b/",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
